{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Рекуррентная нейронная сеть</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекуррентная нейронная сеть (RNN) - тип нейронной сети, где основной идеей является постепенное распространение информации. Используется для обработки временных рядов и текста. Так раз идеально подходит для нашей ситуации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации такой нейросети я буду использовать Keras и вспомогающие функции и классы sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# layers for nntw\n",
    "from tensorflow.keras.layers import Flatten, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# preprocessing and metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также вернём функцию-генератор для загрузки данных и получим тестовые и тренировочные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(path:str):\n",
    "    '''Function, that generates data from path\\control and path\\patient.'''\n",
    "    \n",
    "    # generating control objects\n",
    "    for file in os.listdir(os.path.join(path, 'control')):\n",
    "    \n",
    "        # building full-name of file\n",
    "        full_file = os.path.join(path, os.path.join('control', file))\n",
    "        \n",
    "        # reading .csv file\n",
    "        df = pd.read_csv(full_file)\n",
    "        \n",
    "        # generating activity of object and label\n",
    "        yield df['activity'].values, 0\n",
    "        \n",
    "    \n",
    "    # generating patient objects\n",
    "    for file in os.listdir(os.path.join(path, 'patient')):\n",
    "    \n",
    "        # building full-name of file\n",
    "        full_file = os.path.join(path, os.path.join('patient', file))\n",
    "        \n",
    "        # reading .csv file\n",
    "        df = pd.read_csv(full_file)\n",
    "        \n",
    "        # generating activity of object and label\n",
    "        yield df['activity'].values, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 1, 20160)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [pair for pair in gen_data('data')]\n",
    "\n",
    "# X set with values of acitivity\n",
    "X = np.array([pair[0] for pair in data])\n",
    "\n",
    "# creating normalizer\n",
    "normalizer = Normalizer()\n",
    "X = normalizer.fit_transform(X)\n",
    "\n",
    "# reshape X for LSTm\n",
    "X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Y set with labels\n",
    "Y = np.array([pair[1] for pair in data])\n",
    "\n",
    "# we don't need this list \n",
    "del data\n",
    "\n",
    "# splitting data to train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, shuffle=True, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build():\n",
    "    '''Function, that builds RNN with Keras.'''\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first LSTM layer\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    \n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    # last output layer (sigmoid for binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model for binary classification\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 29 samples, validate on 8 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "29/29 [==============================] - 3s 112ms/sample - loss: 0.6931 - acc: 0.6897 - val_loss: 0.6888 - val_acc: 0.6250\n",
      "Epoch 2/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.6884 - acc: 0.5172 - val_loss: 0.6946 - val_acc: 0.3750\n",
      "Epoch 3/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.6835 - acc: 0.6207 - val_loss: 0.6851 - val_acc: 0.6250\n",
      "Epoch 4/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.6723 - acc: 1.0000 - val_loss: 0.6829 - val_acc: 1.0000\n",
      "Epoch 5/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.6559 - acc: 1.0000 - val_loss: 0.6758 - val_acc: 1.0000\n",
      "Epoch 6/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.6353 - acc: 1.0000 - val_loss: 0.6799 - val_acc: 0.3750\n",
      "Epoch 7/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.6142 - acc: 1.0000 - val_loss: 0.6548 - val_acc: 0.6250\n",
      "Epoch 8/25\n",
      "29/29 [==============================] - 0s 5ms/sample - loss: 0.5941 - acc: 1.0000 - val_loss: 0.6733 - val_acc: 0.3750\n",
      "Epoch 9/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.5656 - acc: 1.0000 - val_loss: 0.6387 - val_acc: 0.7500\n",
      "Epoch 10/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.5279 - acc: 1.0000 - val_loss: 0.6503 - val_acc: 0.6250\n",
      "Epoch 11/25\n",
      "29/29 [==============================] - 0s 5ms/sample - loss: 0.4941 - acc: 1.0000 - val_loss: 0.6076 - val_acc: 0.7500\n",
      "Epoch 12/25\n",
      "29/29 [==============================] - 0s 5ms/sample - loss: 0.4685 - acc: 1.0000 - val_loss: 0.6301 - val_acc: 0.7500\n",
      "Epoch 13/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.4234 - acc: 1.0000 - val_loss: 0.5776 - val_acc: 0.7500\n",
      "Epoch 14/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.3919 - acc: 1.0000 - val_loss: 0.5963 - val_acc: 0.8750\n",
      "Epoch 15/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.3468 - acc: 1.0000 - val_loss: 0.5431 - val_acc: 1.0000\n",
      "Epoch 16/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.3146 - acc: 1.0000 - val_loss: 0.5681 - val_acc: 0.8750\n",
      "Epoch 17/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.2789 - acc: 1.0000 - val_loss: 0.5059 - val_acc: 1.0000\n",
      "Epoch 18/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.2465 - acc: 1.0000 - val_loss: 0.5248 - val_acc: 0.8750\n",
      "Epoch 19/25\n",
      "29/29 [==============================] - 0s 5ms/sample - loss: 0.2165 - acc: 1.0000 - val_loss: 0.4746 - val_acc: 1.0000\n",
      "Epoch 20/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.1893 - acc: 1.0000 - val_loss: 0.4838 - val_acc: 1.0000\n",
      "Epoch 21/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.1671 - acc: 1.0000 - val_loss: 0.4446 - val_acc: 1.0000\n",
      "Epoch 22/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.1470 - acc: 1.0000 - val_loss: 0.4457 - val_acc: 1.0000\n",
      "Epoch 23/25\n",
      "29/29 [==============================] - 0s 5ms/sample - loss: 0.1302 - acc: 1.0000 - val_loss: 0.4186 - val_acc: 1.0000\n",
      "Epoch 24/25\n",
      "29/29 [==============================] - 0s 7ms/sample - loss: 0.1156 - acc: 1.0000 - val_loss: 0.4175 - val_acc: 1.0000\n",
      "Epoch 25/25\n",
      "29/29 [==============================] - 0s 6ms/sample - loss: 0.1032 - acc: 1.0000 - val_loss: 0.3909 - val_acc: 1.0000\n",
      "Test accuracy score is 1.0\n",
      "Train accuracy score is 1.0\n"
     ]
    }
   ],
   "source": [
    "# creating model \n",
    "model = build()\n",
    "\n",
    "# fitting model\n",
    "model.fit(X_train, Y_train, epochs=25, validation_data=[X_test, Y_test])\n",
    "\n",
    "# predictions for test data\n",
    "test_predicted = np.reshape(model.predict(X_test), X_test.shape[0])\n",
    "test_predicted = np.array([1 if value > 0.5 else 0 for value in test_predicted])\n",
    "\n",
    "# predictions for train data\n",
    "train_predicted = np.reshape(model.predict(X_train), X_train.shape[0])\n",
    "train_predicted = np.array([1 if value > 0.5 else 0 for value in train_predicted])\n",
    "\n",
    "# output\n",
    "print(f'Test accuracy score is {accuracy_score(test_predicted, Y_test)}')\n",
    "print(f'Train accuracy score is {accuracy_score(train_predicted, Y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хоть модель и предсказывает правильно 100% тестовых данных (что тоже не является полным показателем успешности модели), однако нам гораздо важнее вероятность того, что человек болен. Кстати, именно поэтому я не использовал f1-score в качестве метрики оценивания. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь дообучим модель на тестовых данных и сохраним нормализатор и модель в файл для их дальнейшего использования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score is 1.0\n",
      "Train accuracy score is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['normalizer.saved']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial fitting on test data\n",
    "model.train_on_batch(X_test, Y_test)\n",
    "\n",
    "# output\n",
    "print(f'Test accuracy score is {accuracy_score(test_predicted, Y_test)}')\n",
    "print(f'Train accuracy score is {accuracy_score(train_predicted, Y_train)}')\n",
    "\n",
    "# saving keras model to .h5 file, using h5 module\n",
    "model.save('model.h5')\n",
    "\n",
    "# saving normalizer using joblib\n",
    "joblib.dump(normalizer, 'normalizer.saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы будем использовать эту модель глубокого обучения и нормализатор в рабочем скрипте predict.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
